{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Pipeline de Dados - F\u00f3rmula 1","text":""},{"location":"#introducao","title":"\ud83d\udc4b Introdu\u00e7\u00e3o","text":"<p>Este projeto foi desenvolvido como parte da disciplina de Engenharia de Dados do curso da SATC. Utilizamos dados hist\u00f3ricos da F\u00f3rmula 1 disponibilizados no Kaggle para a constru\u00e7\u00e3o de um pipeline de dados completo, desde a ingest\u00e3o at\u00e9 a visualiza\u00e7\u00e3o de KPIs em um modelo dimensional.</p>"},{"location":"#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Desenvolver uma pipeline de ingest\u00e3o, transforma\u00e7\u00e3o e an\u00e1lise de dados da F\u00f3rmula 1, com foco em visualiza\u00e7\u00f5es que destacam os pilotos mais r\u00e1pidos por pista e as pistas com maior n\u00famero de acidentes.</p>"},{"location":"#equipe","title":"\ud83d\udc65 Equipe","text":"<ul> <li>Ana Beatriz</li> <li>Iuri</li> <li>Julia </li> <li>Lucas</li> <li>William</li> </ul>"},{"location":"#tecnologias-utilizadas","title":"\ud83e\uddf0 Tecnologias Utilizadas","text":"<ul> <li>Azure Data Lake: Armazenamento de dados em nuvem</li> <li>Databricks: Processamento de dados com Apache Spark</li> <li>Delta Lake: Gerenciamento de dados com transa\u00e7\u00f5es ACID</li> <li>Python e PySpark</li> <li>Terraform: Provisiona recursos na nuvem.</li> <li>SQL Server: Banco de dados relacional</li> <li>MkDocs + Material: Documenta\u00e7\u00e3o do projeto</li> </ul>"},{"location":"conclusao/","title":"Conclus\u00e3o","text":"<p>Este projeto demonstrou a constru\u00e7\u00e3o de uma pipeline de dados completa, do processamento inicial at\u00e9 a gera\u00e7\u00e3o de indicadores de desempenho, utilizando dados reais da F\u00f3rmula 1.</p>"},{"location":"conclusao/#resultados-alcancados","title":"Resultados Alcan\u00e7ados","text":"<ul> <li>Estrutura\u00e7\u00e3o de um pipeline com arquitetura em camadas (Landing \u2192 Bronze \u2192 Silver \u2192 Gold).</li> <li>Constru\u00e7\u00e3o de um modelo dimensional focado em KPIs relevantes.</li> <li>Uso de ferramentas modernas de engenharia de dados: Azure Data Lake, Databricks, Delta Lake, Terraform.</li> </ul>"},{"location":"configuracao-ambiente/","title":"\u2699\ufe0f Configura\u00e7\u00e3o do Ambiente","text":"<p>Esta p\u00e1gina descreve como configurar as ferramentas e servi\u00e7os necess\u00e1rios para executar o pipeline de dados.</p>"},{"location":"configuracao-ambiente/#servicos-e-ferramentas-utilizados","title":"Servi\u00e7os e ferramentas utilizados","text":"<ul> <li>Azure Data Lake Storage Gen2</li> <li>Azure Databricks</li> <li>SQL Server</li> <li>Terraform</li> <li>Python 3.x </li> <li>Azure CLI (para autentica\u00e7\u00e3o e gerenciamento de recursos)</li> <li>Git + GitHub</li> </ul>"},{"location":"configuracao-ambiente/#passo-a-passo-para-configuracao-da-estrutura-do-projeto","title":"Passo a Passo para Configura\u00e7\u00e3o da Estrutura do Projeto","text":"<ol> <li> <p>Clonar o reposit\u00f3rio do projeto    Execute o comando abaixo para clonar este reposit\u00f3rio em sua m\u00e1quina local:    <pre><code>git clone https://github.com/iurilimamarques/projeto-final-engenharia-dados.git\n</code></pre></p> </li> <li> <p>Provisionar a infraestrutura via Terraform    O Terraform do projeto j\u00e1 contempla a cria\u00e7\u00e3o dos principais recursos, incluindo a inst\u00e2ncia do banco de dados SQL Server e a conta de armazenamento do Data Lake.    Execute os comandos abaixo dentro do diret\u00f3rio onde est\u00e3o os arquivos do Terraform:    <pre><code>terraform init\nterraform validate\nterraform fmt\nterraform plan\nterraform apply\n</code></pre></p> </li> <li> <p>Executar os scripts de cria\u00e7\u00e3o e inser\u00e7\u00e3o de tabelas    Ap\u00f3s a cria\u00e7\u00e3o da infraestrutura, localize os scripts SQL no reposit\u00f3rio (possivelmente na pasta <code>sql/</code> ou similar) e execute-os na inst\u00e2ncia do SQL Server provisionada para criar as tabelas e inserir os dados necess\u00e1rios.</p> </li> <li> <p>Provisionar um cluster do Databricks    Crie uma conta no Databricks e provisione um cluster dentro do workspace associado \u00e0 sua assinatura Azure.</p> </li> <li> <p>Integrar o Databricks com o Azure Data Lake    Realize a integra\u00e7\u00e3o entre o Databricks e o Azure Data Lake, garantindo que o cluster Databricks tenha permiss\u00e3o de acesso ao armazenamento criado.</p> </li> <li> <p>Executar a pipeline de dados    Siga as instru\u00e7\u00f5es presentes na se\u00e7\u00e3o Pipeline de Dados para rodar a pipeline, movimentando e processando os dados conforme a arquitetura do projeto.</p> </li> </ol>"},{"location":"modelo-ER/","title":"Modelo Entidade-Relacionamento (ER)","text":""},{"location":"modelo-ER/#diagrama-er","title":"Diagrama ER","text":"<p>Abaixo est\u00e1 o diagrama que representa graficamente o modelo entidade-relacionamento com suas tabelas e relacionamentos:</p> <p></p> <p>\ud83d\udca1 O modelo foi criado com base nos dados dispon\u00edveis no dataset.</p>"},{"location":"modelo-ER/#tabelas","title":"Tabelas","text":"<ul> <li>drivers: Informa\u00e7\u00f5es sobre os pilotos.</li> <li>constructors: Informa\u00e7\u00f5es sobre as equipes (construtores).</li> <li>races: Cada corrida realizada no campeonato, com refer\u00eancia ao circuito, data e ano.</li> <li>circuits: Dados sobre os circuitos onde as corridas ocorrem.</li> <li>results: Resultado de cada piloto em uma corrida.</li> <li>lap_times: Tempos de cada volta de cada piloto em cada corrida.</li> <li>pit_stops: Informa\u00e7\u00f5es sobre as paradas nos boxes.</li> <li>status: Classifica\u00e7\u00f5es do resultado de uma corrida (ex: <code>Finished</code>, <code>Collision</code>, <code>Accident</code>, etc).</li> <li>constructor_results: Resultados por equipe em cada corrida, com pontos obtidos.</li> <li>constructor_standings: Classifica\u00e7\u00e3o final das equipes no campeonato, com pontos, posi\u00e7\u00e3o e vit\u00f3rias.</li> <li>qualifying: Tempos registrados nas sess\u00f5es classificat\u00f3rias que determinam o grid de largada.</li> <li>driver_standings: Classifica\u00e7\u00e3o final dos pilotos no campeonato, com pontua\u00e7\u00e3o, vit\u00f3rias e posi\u00e7\u00e3o.</li> </ul> <p>As tabelas foram criadas em um banco SQL Server, com suas respectivas chaves prim\u00e1rias e estrangeiras. Essa base foi utilizada como fonte da camada landing na pipeline de dados.</p>"},{"location":"modelo-dimensional/","title":"Modelo Dimensional","text":""},{"location":"modelo-dimensional/#estrutura-do-modelo","title":"Estrutura do Modelo","text":"<p>A modelagem foi realizada com base nos dados da camada Gold, processados e organizados no Databricks.</p> <p>\ud83d\udca1 Abaixo est\u00e1 o diagrama representando as tabelas fato e suas dimens\u00f5es:</p> <p></p>"},{"location":"modelo-dimensional/#fato-fat_driver_history","title":"Fato: <code>fat_driver_history</code>","text":"<p>Representa o hist\u00f3rico de voltas de cada piloto por circuito.</p>"},{"location":"modelo-dimensional/#fato-fat_crash_history","title":"Fato: <code>fat_crash_history</code>","text":"<p>Armazena os registros de acidentes e colis\u00f5es por piloto e pista.</p>"},{"location":"modelo-dimensional/#dimensoes","title":"Dimens\u00f5es","text":""},{"location":"modelo-dimensional/#dim_driver","title":"<code>dim_driver</code>","text":"<p>Cont\u00e9m os dados dos pilotos.</p>"},{"location":"modelo-dimensional/#dim_circuits","title":"<code>dim_circuits</code>","text":"<p>Cont\u00e9m os dados dos circuitos.</p>"},{"location":"modelo-dimensional/#dim_races","title":"<code>dim_races</code>","text":"<p>Representa as corridas do campeonato.</p>"},{"location":"modelo-dimensional/#dim_status","title":"<code>dim_status</code>","text":"<p>Classifica\u00e7\u00f5es finais dos pilotos em uma corrida.</p>"},{"location":"visao-geral/","title":"Arquitetura","text":"<p>Esta se\u00e7\u00e3o descreve o passo a passo completo da pipeline de forma geral.</p> <p></p>"},{"location":"visao-geral/#1-preparacao-dos-dados","title":"1. Prepara\u00e7\u00e3o dos Dados","text":"<ul> <li>Realizamos o download do dataset hist\u00f3rico de corridas da F\u00f3rmula 1 no Kaggle.</li> <li>Exploramos os arquivos CSV para entender as tabelas dispon\u00edveis e os relacionamentos entre elas.</li> </ul>"},{"location":"visao-geral/#2-modelagem-relacional","title":"2. Modelagem Relacional","text":"<ul> <li>A partir da an\u00e1lise, constru\u00edmos um modelo entidade-relacionamento (ER)</li> <li>Criamos scripts SQL para:</li> <li>Cria\u00e7\u00e3o das tabelas no SQL Server.</li> <li>Inser\u00e7\u00e3o dos dados a partir dos arquivos CSV.</li> </ul>"},{"location":"visao-geral/#3-instanciar-banco-relacional","title":"3. Instanciar Banco Relacional","text":"<ul> <li>Instanciamos um banco de dados SQL Server na nuvem, com as tabelas populadas.</li> <li>Esse banco relacional serviu como fonte para a extra\u00e7\u00e3o de dados bruta da pipeline.</li> </ul>"},{"location":"visao-geral/#4-armazenamento-em-nuvem-azure-data-lake","title":"4. Armazenamento em Nuvem: Azure Data Lake","text":"<ul> <li>Criamos uma conta de armazenamento no Azure Data Lake Gen2.</li> <li>Estruturamos os dados nas seguintes camadas conforme a arquitetura em formato de medalh\u00e3o:</li> <li><code>landing</code>: Dados brutos extra\u00eddos do SQL Server.</li> <li><code>bronze</code>: Dados organizados por pastas/tabelas, mas ainda sem transforma\u00e7\u00f5es profundas.</li> <li><code>silver</code>: Dados limpos, com joins, nomes padronizados e tipos convertidos.</li> <li><code>gold</code>: Tabelas anal\u00edticas prontas para visualiza\u00e7\u00e3o e explora\u00e7\u00e3o (fatos e dimens\u00f5es).</li> </ul>"},{"location":"visao-geral/#5-processamento-com-databricks-pyspark","title":"5. Processamento com Databricks + PySpark","text":"<ul> <li>Utilizamos o Azure Databricks para construir os notebooks respons\u00e1veis por:</li> <li>Ingest\u00e3o dos dados de cada camada do Data Lake.</li> <li>Aplica\u00e7\u00e3o das transforma\u00e7\u00f5es.</li> <li>Constru\u00e7\u00e3o do modelo dimensional com tabelas de fatos e dimens\u00f5es.</li> </ul>"},{"location":"visao-geral/#6-visualizacao-de-dados","title":"6. Visualiza\u00e7\u00e3o de Dados","text":"<ul> <li>Utilizando o Databricks  constru\u00edmos dashboards que apresentam:</li> <li>Os pilotos mais r\u00e1pidos por circuito ao longo dos anos.</li> <li>As pistas com maior n\u00famero de acidentes e colis\u00f5es.</li> </ul>"},{"location":"visao-geral/#7-provisionamento-com-terraform","title":"7. Provisionamento com Terraform","text":"<ul> <li>Algumas partes do ambiente (como a cria\u00e7\u00e3o do Azure Data Lake) foram automatizadas com Terraform, permitindo versionamento e reuso da infraestrutura.</li> </ul>"},{"location":"pipeline/bronze/","title":"Camada Bronze","text":"<p>A camada Bronze realiza a leitura dos arquivos da Landing e os transforma em tabelas Delta.</p>"},{"location":"pipeline/bronze/#mostrando-todos-os-arquivos-da-camada-landing-zone","title":"Mostrando todos os arquivos da camada landing-zone","text":"<pre><code>display(dbutils.fs.ls(f\"/mnt/{storageAccountName}/landing-zone\"))\n</code></pre>"},{"location":"pipeline/bronze/#gerando-um-dataframe-para-cada-arquivo-a-partir-dos-arquivos-csv-gravado-no-container-landing-zone-do-azure-data-lake-storage","title":"Gerando um dataframe para cada arquivo a partir dos arquivos CSV gravado no container landing-zone do Azure Data Lake Storage","text":"<pre><code>df_circuits = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"/mnt/{storageAccountName}/landing-zone/circuits.csv\")\ndf_constructor_results   = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"/mnt/{storageAccountName}/landing-zone/constructor_results.csv\")\ndf_constructor_standings = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"/mnt/{storageAccountName}/landing-zone/constructor_standings.csv\")\ndf_constructors     = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"/mnt/{storageAccountName}/landing-zone/constructors.csv\")\ndf_driver_standings = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"/mnt/{storageAccountName}/landing-zone/driver_standings.csv\")\ndf_drivers    = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"/mnt/{storageAccountName}/landing-zone/drivers.csv\")\ndf_lap_times  = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"/mnt/{storageAccountName}/landing-zone/lap_times.csv\")\ndf_pit_stops  = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"/mnt/{storageAccountName}/landing-zone/pit_stops.csv\")\ndf_qualifying = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"/mnt/{storageAccountName}/landing-zone/qualifying.csv\")\ndf_races   = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"/mnt/{storageAccountName}/landing-zone/races.csv\")\ndf_results = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"/mnt/{storageAccountName}/landing-zone/results.csv\")\ndf_seasons = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"/mnt/{storageAccountName}/landing-zone/seasons.csv\")\ndf_sprint_results = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"/mnt/{storageAccountName}/landing-zone/sprint_results.csv\")\ndf_status  = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"/mnt/{storageAccountName}/landing-zone/status.csv\")\n</code></pre>"},{"location":"pipeline/bronze/#adicionando-metadados-de-data-e-hora-de-processamento-e-nome-do-arquivo-de-origem","title":"Adicionando metadados de data e hora de processamento e nome do arquivo de origem","text":"<pre><code>from pyspark.sql.functions import current_timestamp, lit\n\ndf_circuits = df_circuits.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"circuits.csv\"))\ndf_constructor_results   = df_constructor_results.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"constructor_results.csv\"))\ndf_constructor_standings = df_constructor_standings.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"constructor_standings.csv\"))\ndf_constructors  = df_constructors.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"constructors.csv\"))\ndf_driver_standings = df_driver_standings.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"driver_standings.csv\"))\ndf_drivers       = df_drivers.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"drivers.csv\"))\ndf_lap_times     = df_lap_times.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"lap_times.csv\"))\ndf_pit_stops     = df_pit_stops.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"pit_stops.csv\"))\ndf_qualifying    = df_qualifying.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"qualifying.csv\"))\ndf_races         = df_races.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"races.csv\"))\ndf_results       = df_results.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"results.csv\"))\ndf_seasons       = df_seasons.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"seasons.csv\"))\ndf_sprint_results = df_sprint_results.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"sprint_results.csv\"))\ndf_status        = df_status.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"status.csv\"))\n</code></pre>"},{"location":"pipeline/bronze/#salvando-os-dataframes-em-delta-lake-formato-de-arquivo-no-data-lake-repositorio-cloud","title":"Salvando os dataframes em delta lake (formato de arquivo) no data lake (repositorio cloud)","text":"<pre><code>df_circuits.write.format('delta').save(f\"/mnt/{storageAccountName}/bronze/circuits\")\ndf_constructor_results.write.format('delta').save(f\"/mnt/{storageAccountName}/bronze/constructor_results\")\ndf_constructor_standings.write.format('delta').save(f\"/mnt/{storageAccountName}/bronze/constructor_standings\")\ndf_constructors.write.format('delta').save(f\"/mnt/{storageAccountName}/bronze/constructors\")\ndf_driver_standings.write.format('delta').save(f\"/mnt/{storageAccountName}/bronze/driver_standings\")\ndf_drivers.write.format('delta').save(f\"/mnt/{storageAccountName}/bronze/drivers\")\ndf_lap_times.write.format('delta').save(f\"/mnt/{storageAccountName}/bronze/lap_times\")\ndf_pit_stops.write.format('delta').save(f\"/mnt/{storageAccountName}/bronze/pit_stops\")\ndf_qualifying.write.format('delta').save(f\"/mnt/{storageAccountName}/bronze/qualifying\")\ndf_races.write.format('delta').save(f\"/mnt/{storageAccountName}/bronze/races\")\ndf_results.write.format('delta').save(f\"/mnt/{storageAccountName}/bronze/results\")\ndf_seasons.write.format('delta').save(f\"/mnt/{storageAccountName}/bronze/seasons\")\ndf_sprint_results.write.format('delta').save(f\"/mnt/{storageAccountName}/bronze/sprint_results\")\ndf_status.write.format('delta').save(f\"/mnt/{storageAccountName}/bronze/status\")\n</code></pre>"},{"location":"pipeline/bronze/#verificando-os-dados-gravados-em-delta-na-camada-bronze","title":"Verificando os dados gravados em delta na camada bronze","text":"<pre><code>display(dbutils.fs.ls(f\"/mnt/{storageAccountName}/bronze/\"))\n</code></pre>"},{"location":"pipeline/bronze/#lendo-um-exemplo-de-um-delta-lake-para-validar-a-existencia-dos-dados-e-das-colunas-do-metadados","title":"Lendo um exemplo de um delta lake para validar a existencia dos dados e das colunas do metadados","text":"<pre><code>spark.read.format('delta').load(f'/mnt/{storageAccountName}/bronze/drivers').limit(10).display()\n</code></pre>"},{"location":"pipeline/gold/","title":"Camada Gold","text":"<p>A camada Gold representa a etapa final de refinamento e transforma\u00e7\u00e3o dos dados, onde eles s\u00e3o preparados para an\u00e1lises avan\u00e7adas e tomadas de decis\u00e3o. </p>"},{"location":"pipeline/gold/#mostrando-todos-os-arquivos-da-camada-silver","title":"Mostrando todos os arquivos da camada silver","text":"<pre><code>display(dbutils.fs.ls(f\"/mnt/{storageAccountName}/silver/\"))\n</code></pre>"},{"location":"pipeline/gold/#gerando-um-dataframe-dos-delta-lake-no-container-silver-do-azure-data-lake-storage","title":"Gerando um dataframe dos delta lake no container silver do Azure Data Lake Storage","text":"<pre><code>df_circuits              = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/circuits\")\ndf_constructor_results   = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/constructor_results\")\ndf_constructor_standings = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/constructor_standings\")\ndf_constructors          = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/constructors\")\ndf_driver_standings      = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/driver_standings\")\ndf_drivers               = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/drivers\")\ndf_lap_times             = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/lap_times\")\ndf_pit_stops             = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/pit_stops\")\ndf_qualifying            = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/qualifying\")\ndf_races                 = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/races\")\ndf_results               = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/results\")\ndf_seasons               = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/seasons\")\ndf_sprint_results        = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/sprint_results\")\ndf_status                = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/status\")\n</code></pre>"},{"location":"pipeline/gold/#criacao-das-tabelas-do-modelo-relacional","title":"Cria\u00e7\u00e3o das tabelas do modelo relacional","text":""},{"location":"pipeline/gold/#criacao-tabela-dim_driver","title":"Cria\u00e7\u00e3o tabela dim_driver","text":"<pre><code>%sql\nUSE spark_catalog.default;\n\nCREATE TABLE IF NOT EXISTS dim_driver (\n\u00a0\u00a0\u00a0 SK_DRIVER BIGINT GENERATED BY DEFAULT AS IDENTITY,\n\u00a0\u00a0\u00a0 IDENTIFIER_DRIVER INTEGER,\n\u00a0\u00a0\u00a0 REFERENCE_DRIVER VARCHAR(100),\n\u00a0\u00a0\u00a0 FORENAME VARCHAR(50),\n\u00a0\u00a0\u00a0 NATIONALITY VARCHAR(50)\n)\nUSING delta\nLOCATION 'dbfs:/mnt/datalakeea44c854c651c494/gold/dim_driver'\n</code></pre>"},{"location":"pipeline/gold/#insercao-de-dados-na-tabela-dim_driver","title":"Inser\u00e7\u00e3o de dados na tabela dim_driver","text":"<pre><code>%sql\n\nWITH drivers_relacional AS (\n  SELECT \n    try_cast(IDENTIFIER_DRIVER AS INT) AS IDENTIFIER_DRIVER,\n    REFERENCE_DRIVER,\n    FORENAME,\n    NATIONALITY\n  FROM drivers\n)\n\nMERGE INTO dim_driver AS d\nUSING drivers_relacional AS dr\nON d.IDENTIFIER_DRIVER = dr.IDENTIFIER_DRIVER\n\nWHEN MATCHED AND (\n     d.REFERENCE_DRIVER &lt;&gt; dr.REFERENCE_DRIVER OR\n     d.FORENAME &lt;&gt; dr.FORENAME OR\n     d.NATIONALITY &lt;&gt; dr.NATIONALITY\n) THEN\n  UPDATE SET \n    REFERENCE_DRIVER = dr.REFERENCE_DRIVER,\n    FORENAME = dr.FORENAME,\n    NATIONALITY = dr.NATIONALITY\n\nWHEN NOT MATCHED THEN\n  INSERT (IDENTIFIER_DRIVER, REFERENCE_DRIVER, FORENAME, NATIONALITY)\n  VALUES (dr.IDENTIFIER_DRIVER, dr.REFERENCE_DRIVER, dr.FORENAME, dr.NATIONALITY);\n</code></pre>"},{"location":"pipeline/gold/#criacao-tabela-dim_circuits","title":"Cria\u00e7\u00e3o tabela dim_circuits","text":"<pre><code>%sql\nUSE spark_catalog.default;\n\nCREATE TABLE IF NOT EXISTS dim_circuits (\n\u00a0\u00a0\u00a0 SK_CIRCUITS BIGINT GENERATED BY DEFAULT AS IDENTITY,\n\u00a0\u00a0\u00a0 IDENTIFIER_CIRCUIT INTEGER,\n    REFERENCE_CIRCUIT VARCHAR(100),\n\u00a0\u00a0\u00a0 COUNTRY VARCHAR(50)\n)\nUSING delta\nLOCATION 'dbfs:/mnt/datalakeea44c854c651c494/gold/dim_circuits'\n</code></pre>"},{"location":"pipeline/gold/#insercao-de-dados-na-tabela-dim_circuits","title":"Inser\u00e7\u00e3o de dados na tabela dim_circuits","text":"<pre><code>%sql\nWITH circuits_relacional AS (\n  SELECT \n    try_cast(IDENTIFIER_CIRCUIT AS INT) AS IDENTIFIER_CIRCUIT,\n    REFERENCE_CIRCUIT,\n    COUNTRY\n  FROM circuits\n)\n\nMERGE INTO dim_circuits AS dc\nUSING circuits_relacional AS cr\nON dc.IDENTIFIER_CIRCUIT = cr.IDENTIFIER_CIRCUIT\n\nWHEN MATCHED AND (\n     dc.REFERENCE_CIRCUIT &lt;&gt; cr.REFERENCE_CIRCUIT OR\n     dc.COUNTRY &lt;&gt; cr.COUNTRY\n) THEN\n  UPDATE SET \n    REFERENCE_CIRCUIT = cr.REFERENCE_CIRCUIT,\n    COUNTRY = cr.COUNTRY\n\nWHEN NOT MATCHED THEN\n  INSERT (IDENTIFIER_CIRCUIT, REFERENCE_CIRCUIT, COUNTRY)\n  VALUES (cr.IDENTIFIER_CIRCUIT, cr.REFERENCE_CIRCUIT, cr.COUNTRY);\n</code></pre>"},{"location":"pipeline/gold/#criacao-tabela-dim_status","title":"Cria\u00e7\u00e3o tabela dim_status","text":"<pre><code>%sql\nUSE spark_catalog.default;\n\nCREATE TABLE IF NOT EXISTS dim_status (\n\u00a0\u00a0\u00a0 SK_STATUS BIGINT GENERATED BY DEFAULT AS IDENTITY,\n\u00a0\u00a0\u00a0 IDENTIFIER_STATUS INTEGER,\n\u00a0\u00a0\u00a0 STATUS VARCHAR(20)\n)\nUSING delta\nLOCATION 'dbfs:/mnt/datalakeea44c854c651c494/gold/dim_status'\n</code></pre>"},{"location":"pipeline/gold/#insercao-de-dados-na-tabela-dim_status","title":"Inser\u00e7\u00e3o de dados na tabela dim_status","text":"<pre><code>%sql\nWITH status_relacional AS (\n  SELECT \n    try_cast(IDENTIFIER_STATUS AS INT) AS IDENTIFIER_STATUS,\n    STATUS\n  FROM status\n)\n\nMERGE INTO dim_status AS ds\nUSING status_relacional AS sr\nON ds.IDENTIFIER_STATUS = sr.IDENTIFIER_STATUS\n\nWHEN MATCHED AND (\n     ds.STATUS &lt;&gt; sr.STATUS\n) THEN\n  UPDATE SET \n    ds.STATUS = sr.STATUS\n\nWHEN NOT MATCHED THEN\n  INSERT (IDENTIFIER_STATUS, STATUS)\n  VALUES (sr.IDENTIFIER_STATUS, sr.STATUS);\n</code></pre>"},{"location":"pipeline/gold/#criacao-tabela-dim_time","title":"Cria\u00e7\u00e3o tabela dim_time","text":"<pre><code>from pyspark.sql.functions import expr, date_format, row_number\nfrom pyspark.sql.window import Window\n\n# Define o intervalo de datas desejado\ndata_inicial = \"1950-05-12\"\ndata_final = \"2024-12-07\"\n\n# Calcula o n\u00famero de dias no intervalo\nnum_dias = spark.sql(f\"SELECT datediff('{data_final}', '{data_inicial}')\").collect()[0][0]\n\n# Cria um DataFrame com uma coluna contendo uma sequ\u00eancia de datas\ndf_calendario = spark.range(0, num_dias + 1) \\\n    .selectExpr(f\"date_add(to_date('{data_inicial}'), CAST(id AS INT)) AS Data\")\n\n# Extrai os componentes de data\ndf_tempo = df_calendario.selectExpr(\n    \"Data\",\n    \"year(Data) AS Ano\",\n    \"month(Data) AS Mes\",\n    \"(CASE month(Data) \\\n        WHEN 1 THEN 'JANEIRO' \\\n        WHEN 2 THEN 'FEVEREIRO' \\\n        WHEN 3 THEN 'MARCO' \\\n        WHEN 4 THEN 'ABRIL' \\\n        WHEN 5 THEN 'MAIO' \\\n        WHEN 6 THEN 'JUNHO' \\\n        WHEN 7 THEN 'JULHO' \\\n        WHEN 8 THEN 'AGOSTO' \\\n        WHEN 9 THEN 'SETEMBRO' \\\n        WHEN 10 THEN 'OUTUBRO' \\\n        WHEN 11 THEN 'NOVEMBRO' \\\n        WHEN 12 THEN 'DEZEMBRO' \\\n    END) AS NomeMes\",\n    \"day(Data) AS Dia\",\n    \"(CASE dayofweek(Data) \\\n        WHEN 1 THEN 'DOMINGO' \\\n        WHEN 2 THEN 'SEGUNDA-FEIRA' \\\n        WHEN 3 THEN 'TERCA-FEIRA' \\\n        WHEN 4 THEN 'QUARTA-FEIRA' \\\n        WHEN 5 THEN 'QUINTA-FEIRA' \\\n        WHEN 6 THEN 'SEXTA-FEIRA' \\\n        WHEN 7 THEN 'SABADO' \\\n    END) AS NomeDiaSemana\",\n    \"dayofweek(Data) AS NumeroDiaSemana\"\n)\n\n# Define a janela para gerar o surrogate key\nwindowSpec = Window.orderBy(\"Data\")\n\n# Adiciona a coluna sk_time\ndf_tempo_sk = df_tempo.withColumn(\"sk_time\", row_number().over(windowSpec))\n\n# Exibe o DataFrame resultante\ndf_tempo_sk.display()\n\n# Salva como tabela Delta\ndf_tempo_sk.write.option(\"path\", f\"/mnt/{storageAccountName}/gold/dim_time\").saveAsTable(\"dim_time\", format=\"delta\")\n</code></pre>"},{"location":"pipeline/gold/#criacao-tabela-dim_races","title":"Cria\u00e7\u00e3o tabela dim_races","text":"<pre><code>%sql\nUSE spark_catalog.default;\n\nCREATE TABLE IF NOT EXISTS dim_races (\n  SK_RACES BIGINT GENERATED BY DEFAULT AS IDENTITY,\n  SK_TIME BIGINT,\n  IDENTIFIER_RACE INTEGER\n)\nUSING delta\nLOCATION 'dbfs:/mnt/datalakeea44c854c651c494/gold/dim_races';\n</code></pre>"},{"location":"pipeline/gold/#insercao-de-dados-na-tabela-dim_races","title":"Inser\u00e7\u00e3o de dados na tabela dim_races","text":"<pre><code>%sql\nWITH races_data AS (\n  SELECT\n    IDENTIFIER_RACE,\n    TO_DATE(DATE, 'yyyy-MM-dd') AS race_date\n  FROM\n    races\n),\n\nraces_com_sk_time AS (\n  SELECT\n    r.IDENTIFIER_RACE,\n    dt.sk_time\n  FROM\n    races_data r\n  JOIN\n    dim_time dt\n  ON\n    r.race_date = dt.Data\n)\n\nMERGE INTO dim_races dr\nUSING races_com_sk_time rst\nON dr.IDENTIFIER_RACE = rst.IDENTIFIER_RACE\n\nWHEN NOT MATCHED THEN\n  INSERT (SK_TIME, IDENTIFIER_RACE)\n  VALUES (rst.sk_time, rst.IDENTIFIER_RACE)\n</code></pre>"},{"location":"pipeline/gold/#criacao-tabela-fato-fat_crash_history","title":"Cria\u00e7\u00e3o tabela fato fat_crash_history","text":"<pre><code>%sql\ncreate table fat_crash_history (\n   FK_RACES             int,\n   FK_STATUS            int,\n   FK_DRIVER            int,\n   FK_CIRCUITS          int\n)\nUSING delta\nLOCATION '/mnt/datalakeea44c854c651c494/gold/fat_crash_history'\n</code></pre>"},{"location":"pipeline/gold/#insercao-de-dados-na-tabela-fato-fat_crash_history","title":"Inser\u00e7\u00e3o de dados na tabela fato fat_crash_history","text":"<pre><code>%sql\nCREATE OR REPLACE TABLE fat_crash_history\nUSING delta\nLOCATION '/mnt/datalakeea44c854c651c494/gold/fat_crash_history'\nAS\nSELECT\n  d.SK_DRIVER     AS FK_DRIVER,\n  s.SK_STATUS     AS FK_STATUS,\n  r.SK_RACES      AS FK_RACES,\n  c.SK_CIRCUITS   AS FK_CIRCUITS\nFROM results res\nJOIN races ra \n  ON res.IDENTIFIER_RACE = ra.IDENTIFIER_RACE\nJOIN dim_driver d \n  ON res.IDENTIFIER_DRIVER = d.IDENTIFIER_DRIVER\nJOIN dim_status s \n  ON res.IDENTIFIER_STATUS = s.IDENTIFIER_STATUS\nJOIN dim_races r \n  ON res.IDENTIFIER_RACE = r.IDENTIFIER_RACE\nJOIN dim_circuits c \n  ON ra.IDENTIFIER_CIRCUIT = c.IDENTIFIER_CIRCUIT\nWHERE res.IDENTIFIER_STATUS IN (3, 4);\n</code></pre>"},{"location":"pipeline/gold/#criacao-tabela-fato-fat_driver_history","title":"Cria\u00e7\u00e3o tabela fato fat_driver_history","text":"<pre><code>%sql\ncreate table fat_driver_history (\n   FK_RACES             int,\n   FK_DRIVER            int,\n   FK_CIRCUITS          int,\n   FASTESTLAPTIME       int\n)\nUSING delta\nLOCATION '/mnt/datalakeea44c854c651c494/gold/fat_driver_history'\n</code></pre>"},{"location":"pipeline/gold/#insercao-de-dados-na-tabela-fato-fat_driver_history","title":"Inser\u00e7\u00e3o de dados na tabela fato fat_driver_history","text":"<pre><code>%sql\nCREATE OR REPLACE TABLE fat_driver_history\nUSING delta\nLOCATION '/mnt/datalakeea44c854c651c494/gold/fat_driver_history'\nAS\nSELECT\n  d.SK_DRIVER        AS FK_DRIVER,\n  r.SK_RACES         AS FK_RACES,\n  c.SK_CIRCUITS      AS FK_CIRCUITS,\n  res.FASTESTLAPTIME AS FASTESTLAPTIME\nFROM results res\nJOIN races ra \n  ON res.IDENTIFIER_RACE = ra.IDENTIFIER_RACE\nJOIN dim_driver d \n  ON res.IDENTIFIER_DRIVER = d.IDENTIFIER_DRIVER\nJOIN dim_races r \n  ON res.IDENTIFIER_RACE = r.IDENTIFIER_RACE\nJOIN dim_circuits c \n  ON ra.IDENTIFIER_CIRCUIT = c.IDENTIFIER_CIRCUIT\nWHERE res.FASTESTLAPTIME IS NOT NULL;\n</code></pre>"},{"location":"pipeline/silver/","title":"Camada Silver","text":"<p>A camada Silver \u00e9 respons\u00e1vel pela limpeza e padroniza\u00e7\u00e3o dos dados brutos da camada Bronze, garantindo que os dados estejam estruturados de maneira consistente.</p>"},{"location":"pipeline/silver/#mostrando-todos-os-arquivos-da-camada-bronze","title":"Mostrando todos os arquivos da camada bronze","text":"<pre><code>display(dbutils.fs.ls(f\"/mnt/{storageAccountName}/bronze\"))\n</code></pre>"},{"location":"pipeline/silver/#gerando-um-dataframe-dos-delta-lake-no-container-bronze-do-azure-data-lake-storage","title":"Gerando um dataframe dos delta lake no container bronze do Azure Data Lake Storage","text":"<pre><code>df_circuits   = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/circuits\")\ndf_constructor_results     = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/constructor_results\")\ndf_constructor_standings   = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/constructor_standings\")\ndf_constructors  = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/constructors\")\ndf_driver_standings    = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/driver_standings\")\ndf_drivers     = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/drivers\")\ndf_lap_times     = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/lap_times\")\ndf_pit_stops    = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/pit_stops\")\ndf_qualifying = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/qualifying\")\ndf_races    = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/races\")\ndf_results  = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/results\")\ndf_seasons  = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/seasons\")\ndf_sprint_results  = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/sprint_results\")\ndf_status  = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/status\")\n</code></pre>"},{"location":"pipeline/silver/#adicionando-metadados-de-data-e-hora-de-processamento-e-nome-do-arquivo-de-origem","title":"Adicionando metadados de data e hora de processamento e nome do arquivo de origem","text":"<pre><code>from pyspark.sql.functions import current_timestamp, lit\n\ndf_circuits = df_circuits.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"circuits\"))\ndf_constructor_results   = df_constructor_results.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"constructor_results\"))\ndf_constructor_standings = df_constructor_standings.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"constructor_standings\"))\ndf_constructors  = df_constructors.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"constructors\"))\ndf_driver_standings = df_driver_standings.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"driver_standings\"))\ndf_drivers       = df_drivers.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"drivers\"))\ndf_lap_times     = df_lap_times.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"lap_times\"))\ndf_pit_stops     = df_pit_stops.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"pit_stops\"))\ndf_qualifying    = df_qualifying.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"qualifying\"))\ndf_races         = df_races.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"races\"))\ndf_results       = df_results.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"results\"))\ndf_seasons       = df_seasons.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"seasons\"))\ndf_sprint_results = df_sprint_results.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"sprint_results\"))\ndf_status        = df_status.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"status\"))\n</code></pre>"},{"location":"pipeline/silver/#limpeza-e-padronizacao-dos-dados","title":"Limpeza e padroniza\u00e7\u00e3o dos dados","text":"<pre><code># Mudando o nome das colunas para mai\u00fascula e ajustanto os nomes das colunas de acordo com o dicionario de dados\n\n#['circuitId', 'circuitRef', 'name', 'location', 'country', 'lat', 'lng', 'alt', 'url', 'data_hora_bronze', 'nome_arquivo', 'data_hora_silver'] \ndf_circuits = ( df_circuits \n               .withColumnRenamed(\"circuitId\" , \"IDENTIFIER_CIRCUIT\")\n               .withColumnRenamed(\"circuitRef\" , \"REFERENCE_CIRCUIT\")\n               .withColumnRenamed(\"name\" , \"NAME\")\n               .withColumnRenamed(\"location\" , \"LOCATION\")\n               .withColumnRenamed(\"country\" , \"COUNTRY\" )\n               .withColumnRenamed(\"lat\" , \"LATITUDE\" )\n               .withColumnRenamed(\"lng\" , \"LONGITUDE\" )\n               .withColumnRenamed(\"alt\" , \"ALTITUDE\" )\n               .withColumnRenamed(\"url\" , \"URL\" )\n               .withColumnRenamed(\"data_hora_bronze\", \"DATA_HORA_BRONZE\") \n               .withColumnRenamed(\"nome_arquivo\" , \"NOME_ARQUIVO\") \n               .withColumnRenamed(\"data_hora_silver\" , \"DATA_HORA_SILVER\") \n               )\n#['constructorResultsId', 'raceId', 'constructorId', 'points', 'status', 'data_hora_bronze', 'nome_arquivo', 'data_hora_silver'] \ndf_constructor_results = ( df_constructor_results \n                          .withColumnRenamed(\"constructorResultsId\" , \"IDENTIFIER_CONSTRUCTOR_RESULTS\")\n                          .withColumnRenamed(\"raceId\" , \"IDENTIFIER_RACE\")\n                          .withColumnRenamed(\"constructorId\" , \"IDENTIFIER_CONSTRUCTOR\")\n                          .withColumnRenamed(\"points\" , \"POINTS\")\n                          .withColumnRenamed(\"status\" , \"STATUS\")\n                          .withColumnRenamed(\"data_hora_bronze\", \"DATA_HORA_BRONZE\") \n                          .withColumnRenamed(\"nome_arquivo\" , \"NOME_ARQUIVO\") \n                          .withColumnRenamed(\"data_hora_silver\" , \"DATA_HORA_SILVER\") \n                          )\n#['constructorStandingsId', 'raceId', 'constructorId', 'points', 'position', 'positionText', 'wins', 'data_hora_bronze', 'nome_arquivo', 'data_hora_silver']\ndf_constructor_standings = ( df_constructor_standings \n                            .withColumnRenamed(\"constructorStandingsId\" , \"IDENTIFIER_CONSTRUCTOR_STANDINGS\")\n                            .withColumnRenamed(\"raceId\" , \"IDENTIFIER_RACE\")\n                            .withColumnRenamed(\"constructorId\" , \"IDENTIFIER_CONSTRUCTOR\")\n                            .withColumnRenamed(\"position\" , \"POSITION\")\n                            .withColumnRenamed(\"positionText\" , \"POSITION_TEXT\")\n                            .withColumnRenamed(\"wins\" , \"WINS\")\n                            .withColumnRenamed(\"data_hora_bronze\", \"DATA_HORA_BRONZE\") \n                            .withColumnRenamed(\"nome_arquivo\" , \"NOME_ARQUIVO\") \n                            .withColumnRenamed(\"data_hora_silver\" , \"DATA_HORA_SILVER\") \n                            )\n#['constructorId', 'constructorRef', 'name', 'nationality', 'url', 'data_hora_bronze', 'nome_arquivo', 'data_hora_silver'] \ndf_constructors = ( df_constructors     \n                   .withColumnRenamed(\"constructorId\" , \"IDENTIFIER_CONSTRUCTOR\")\n                   .withColumnRenamed(\"constructorRef\" , \"REFERENCE_CONSTRUCTOR\")\n                   .withColumnRenamed(\"name\" , \"NAME\")\n                   .withColumnRenamed(\"nationality\" , \"NATIONALITY\")\n                   .withColumnRenamed(\"url\" , \"URL\")\n                   .withColumnRenamed(\"data_hora_bronze\", \"DATA_HORA_BRONZE\") \n                   .withColumnRenamed(\"nome_arquivo\" , \"NOME_ARQUIVO\") \n                   .withColumnRenamed(\"data_hora_silver\" , \"DATA_HORA_SILVER\") \n                   )\n#['driverStandingsId', 'raceId', 'driverId', 'points', 'position', 'positionText', 'wins', 'data_hora_bronze', 'nome_arquivo', 'data_hora_silver'] \ndf_driver_standings = ( df_driver_standings \n                       .withColumnRenamed(\"driverStandingsId\" , \"IDENTIFIER_DRIVER_STANDINGS\")\n                       .withColumnRenamed(\"raceId\" , \"IDENTIFIER_RACE\")\n                       .withColumnRenamed(\"driverId\" , \"IDENTIFIER_DRIVER\")\n                       .withColumnRenamed(\"points\" , \"POINTS\")\n                       .withColumnRenamed(\"position\" , \"POSITION\")\n                       .withColumnRenamed(\"positionText\" , \"POSITION_TEXT\")\n                       .withColumnRenamed(\"wins\" , \"WINS\")\n                       .withColumnRenamed(\"data_hora_bronze\", \"DATA_HORA_BRONZE\") \n                       .withColumnRenamed(\"nome_arquivo\" , \"NOME_ARQUIVO\") \n                       .withColumnRenamed(\"data_hora_silver\" , \"DATA_HORA_SILVER\") \n                       )\n#['driverId', 'driverRef', 'number', 'code', 'forename', 'surname', 'dob', 'nationality', 'url', 'data_hora_bronze', 'nome_arquivo', 'data_hora_silver'] \ndf_drivers = ( df_drivers \n              .withColumnRenamed(\"driverId\" , \"IDENTIFIER_DRIVER\")\n              .withColumnRenamed(\"driverRef\" , \"REFERENCE_DRIVER\")\n              .withColumnRenamed(\"number\" , \"NUMBER\")\n              .withColumnRenamed(\"code\" , \"CODE\")\n              .withColumnRenamed(\"forename\" , \"FORENAME\")\n              .withColumnRenamed(\"surname\" , \"SURNAME\")\n              .withColumnRenamed(\"dob\" , \"DOB\")\n              .withColumnRenamed(\"nationality\" , \"NATIONALITY\")\n              .withColumnRenamed(\"url\" , \"URL\")\n              .withColumnRenamed(\"data_hora_bronze\", \"DATA_HORA_BRONZE\") \n              .withColumnRenamed(\"nome_arquivo\" , \"NOME_ARQUIVO\") \n              .withColumnRenamed(\"data_hora_silver\" , \"DATA_HORA_SILVER\") \n              )\n#['raceId', 'driverId', 'lap', 'position', 'time', 'milliseconds', 'data_hora_bronze', 'nome_arquivo', 'data_hora_silver']            \ndf_lap_times = ( df_lap_times \n                .withColumnRenamed(\"RACEID\" , \"IDENTIFIER_RACE\")\n                .withColumnRenamed(\"driverId\" , \"IDENTIFIER_DRIVER\")\n                .withColumnRenamed(\"lap\" , \"LAP\")\n                .withColumnRenamed(\"position\" , \"POSITION\")\n                .withColumnRenamed(\"time\" , \"TIME\")\n                .withColumnRenamed(\"milliseconds\" , \"MILLISECONDS\")\n                .withColumnRenamed(\"data_hora_bronze\", \"DATA_HORA_BRONZE\") \n                .withColumnRenamed(\"nome_arquivo\" , \"NOME_ARQUIVO\") \n                .withColumnRenamed(\"data_hora_silver\" , \"DATA_HORA_SILVER\") \n                )\n#['raceId', 'driverId', 'stop', 'lap', 'time', 'duration', 'milliseconds', 'data_hora_bronze', 'nome_arquivo', 'data_hora_silver']             \ndf_pit_stops = ( df_pit_stops\n                .withColumnRenamed(\"raceId\" , \"IDENTIFIER_RACE\")\n                .withColumnRenamed(\"driverId\" , \"IDENTIFIER_DRIVER\")\n                .withColumnRenamed(\"stop\" , \"STOP\")\n                .withColumnRenamed(\"lap\" , \"LAP\")\n                .withColumnRenamed(\"time\" , \"TIME\")\n                .withColumnRenamed(\"duration\" , \"DURATION\")\n                .withColumnRenamed(\"milliseconds\" , \"MILLISECONDS\")\n                .withColumnRenamed(\"data_hora_bronze\", \"DATA_HORA_BRONZE\") \n                .withColumnRenamed(\"nome_arquivo\" , \"NOME_ARQUIVO\") \n                .withColumnRenamed(\"data_hora_silver\" , \"DATA_HORA_SILVER\") \n                )\n#['qualifyId', 'raceId', 'driverId', 'constructorId', 'number', 'position', 'q1', 'q2', 'q3', 'data_hora_bronze', 'nome_arquivo', 'data_hora_silver'] \ndf_qualifying = ( df_qualifying \n                 .withColumnRenamed(\"qualifyId\" , \"IDENTIFIER_QUALIFYING\")\n                 .withColumnRenamed(\"raceId\" , \"IDENTIFIER_RACE\")\n                 .withColumnRenamed(\"driverId\" , \"IDENTIFIER_DRIVER\")\n                 .withColumnRenamed(\"constructorId\" , \"IDENTIFIER_CONSTRUCTOR\")\n                 .withColumnRenamed(\"number\" , \"NUMBER\")\n                 .withColumnRenamed(\"position\" , \"POSITION\")\n                 .withColumnRenamed(\"q1\" , \"Q1\")\n                 .withColumnRenamed(\"q2\" , \"Q2\")\n                 .withColumnRenamed(\"q3\" , \"Q3\")\n                 .withColumnRenamed(\"data_hora_bronze\", \"DATA_HORA_BRONZE\") \n                 .withColumnRenamed(\"nome_arquivo\" , \"NOME_ARQUIVO\") \n                 .withColumnRenamed(\"data_hora_silver\" , \"DATA_HORA_SILVER\") \n                 )\n#['raceId', 'year', 'round', 'circuitId', 'name', 'date', 'time', 'url', 'fp1_date', 'fp1_time', 'fp2_date', 'fp2_time', 'fp3_date', 'fp3_time', 'quali_date', 'quali_time', 'sprint_date', 'sprint_time', 'data_hora_bronze', 'nome_arquivo', 'data_hora_silver'] \ndf_races = ( df_races \n            .withColumnRenamed(\"RACEID\" , \"IDENTIFIER_RACE\")\n            .withColumnRenamed(\"year\" , \"YEAR\")\n            .withColumnRenamed(\"round\" , \"ROUND\")\n            .withColumnRenamed(\"circuitId\" , \"IDENTIFIER_CIRCUIT\")\n            .withColumnRenamed(\"name\" , \"NAME\")\n            .withColumnRenamed(\"date\" , \"DATE\")\n            .withColumnRenamed(\"time\" , \"TIME\")\n            .withColumnRenamed(\"url\" , \"URL\")\n            .withColumnRenamed(\"fp1_date\" , \"FP1_DATE\")\n            .withColumnRenamed(\"fp1_time\" , \"FP1_TIME\")\n            .withColumnRenamed(\"fp2_date\" , \"FP2_DATE\")\n            .withColumnRenamed(\"fp2_time\" , \"FP2_TIME\")\n            .withColumnRenamed(\"fp3_date\" , \"FP3_DATE\")\n            .withColumnRenamed(\"fp3_time\" , \"FP3_TIME\")\n            .withColumnRenamed(\"quali_date\" , \"QUALI_DATE\")\n            .withColumnRenamed(\"quali_time\" , \"QUALI_TIME\")\n            .withColumnRenamed(\"sprint_date\" , \"SPRINT_DATE\")\n            .withColumnRenamed(\"sprint_time\" , \"SPRINT_TIME\")\n            .withColumnRenamed(\"data_hora_bronze\", \"DATA_HORA_BRONZE\") \n            .withColumnRenamed(\"nome_arquivo\" , \"NOME_ARQUIVO\") \n            .withColumnRenamed(\"data_hora_silver\" , \"DATA_HORA_SILVER\") \n            )\n#['resultId', 'raceId', 'driverId', 'constructorId', 'number', 'grid', 'position', 'positionText', 'positionOrder', 'points', 'laps', 'time', 'milliseconds', 'fastestLap', 'rank', 'fastestLapTime', 'fastestLapSpeed', 'statusId', 'data_hora_bronze', 'nome_arquivo', 'data_hora_silver'] \ndf_results = ( df_results \n              .withColumnRenamed(\"resultId\" , \"IDENTIFIER_RESULT\")\n              .withColumnRenamed(\"raceId\" , \"IDENTIFIER_RACE\")\n              .withColumnRenamed(\"driverId\" , \"IDENTIFIER_DRIVER\")\n              .withColumnRenamed(\"constructorId\" , \"IDENTIFIER_CONSTRUCTOR\")\n              .withColumnRenamed(\"number\" , \"NUMBER\")\n              .withColumnRenamed(\"grid\" , \"GRID\")\n              .withColumnRenamed(\"position\" , \"POSITION\")\n              .withColumnRenamed(\"positionText\" , \"POSITION_TEXT\")\n              .withColumnRenamed(\"positionOrder\" , \"POSITION_ORDER\")\n              .withColumnRenamed(\"points\" , \"POINTS\")\n              .withColumnRenamed(\"laps\" , \"LAPS\")\n              .withColumnRenamed(\"time\" , \"TIME\")\n              .withColumnRenamed(\"milliseconds\" , \"MILLISECONDS\")\n              .withColumnRenamed(\"fastestLap\" , \"FASTESTLAP\")\n              .withColumnRenamed(\"rank\" , \"RANK\")\n              .withColumnRenamed(\"fastestLapTime\" , \"FASTESTLAPTIME\")\n              .withColumnRenamed(\"fastestLapSpeed\" , \"FASTESTLAPSPEED\")\n              .withColumnRenamed(\"statusId\" , \"IDENTIFIER_STATUS\")\n              .withColumnRenamed(\"data_hora_bronze\", \"DATA_HORA_BRONZE\") \n              .withColumnRenamed(\"nome_arquivo\" , \"NOME_ARQUIVO\") \n              .withColumnRenamed(\"data_hora_silver\" , \"DATA_HORA_SILVER\") \n              )\n#['year', 'url', 'data_hora_bronze', 'nome_arquivo', 'data_hora_silver']               \ndf_seasons = (df_seasons \n              .withColumnRenamed(\"year\" , \"YEAR\")\n              .withColumnRenamed(\"url\" , \"URL\")\n              .withColumnRenamed(\"data_hora_bronze\", \"DATA_HORA_BRONZE\") \n              .withColumnRenamed(\"nome_arquivo\" , \"NOME_ARQUIVO\") \n              .withColumnRenamed(\"data_hora_silver\" , \"DATA_HORA_SILVER\")\n              )\n#['resultId', 'raceId', 'driverId', 'constructorId', 'number', 'grid', 'position', 'positionText', 'positionOrder', 'points', 'laps', 'time', 'milliseconds', 'fastestLap', 'fastestLapTime', 'statusId', 'data_hora_bronze', 'nome_arquivo', 'data_hora_silver'] \ndf_sprint_results = ( df_sprint_results\n                     .withColumnRenamed(\"resultId\" , \"IDENTIFIER_RESULT\")\n                     .withColumnRenamed(\"raceId\" , \"IDENTIFIER_RACE\")\n                     .withColumnRenamed(\"driverId\" , \"IDENTIFIER_DRIVER\")\n                     .withColumnRenamed(\"constructorId\" , \"IDENTIFIER_CONSTRUCTOR\")\n                     .withColumnRenamed(\"number\" , \"NUMBER\")\n                     .withColumnRenamed(\"grid\" , \"GRID\")\n                     .withColumnRenamed(\"position\" , \"POSITION\")\n                     .withColumnRenamed(\"positionText\" , \"POSITION_TEXT\")\n                     .withColumnRenamed(\"positionOrder\" , \"POSITION_ORDER\")\n                     .withColumnRenamed(\"points\" , \"POINTS\")\n                     .withColumnRenamed(\"laps\" , \"LAPS\")\n                     .withColumnRenamed(\"time\" , \"TIME\")\n                     .withColumnRenamed(\"milliseconds\" , \"MILLISECONDS\")\n                     .withColumnRenamed(\"fastestLap\" , \"FASTESTLAP\")\n                     .withColumnRenamed(\"fastestLapTime\" , \"FASTESTLAPTIME\")\n                     .withColumnRenamed(\"statusId\" , \"IDENTIFIER_STATUS\")\n                     .withColumnRenamed(\"data_hora_bronze\", \"DATA_HORA_BRONZE\") \n                     .withColumnRenamed(\"nome_arquivo\" , \"NOME_ARQUIVO\") \n                     .withColumnRenamed(\"data_hora_silver\" , \"DATA_HORA_SILVER\")\n                     )\n#['statusId', 'status', 'data_hora_bronze', 'nome_arquivo', 'data_hora_silver']\ndf_status = ( df_status\n            .withColumnRenamed(\"statusId\" , \"IDENTIFIER_STATUS\")\n            .withColumnRenamed(\"status\" , \"STATUS\")\n            .withColumnRenamed(\"data_hora_bronze\", \"DATA_HORA_BRONZE\") \n            .withColumnRenamed(\"nome_arquivo\" , \"NOME_ARQUIVO\") \n            .withColumnRenamed(\"data_hora_silver\" , \"DATA_HORA_SILVER\") \n            )\n</code></pre>"},{"location":"pipeline/silver/#salvando-os-dataframes-em-delta-lake-formato-de-arquivo-no-data-lake-repositorio-cloud","title":"Salvando os dataframes em delta lake (formato de arquivo) no data lake (repositorio cloud)","text":"<pre><code>df_circuits.write.format('delta').save(f\"/mnt/{storageAccountName}/silver/circuits\")\ndf_constructor_results.write.format('delta').save(f\"/mnt/{storageAccountName}/silver/constructor_results\")\ndf_constructor_standings.write.format('delta').save(f\"/mnt/{storageAccountName}/silver/constructor_standings\")\ndf_constructors.write.format('delta').save(f\"/mnt/{storageAccountName}/silver/constructors\")\ndf_driver_standings.write.format('delta').save(f\"/mnt/{storageAccountName}/silver/driver_standings\")\ndf_drivers.write.format('delta').save(f\"/mnt/{storageAccountName}/silver/drivers\")\ndf_lap_times.write.format('delta').save(f\"/mnt/{storageAccountName}/silver/lap_times\")\ndf_pit_stops.write.format('delta').save(f\"/mnt/{storageAccountName}/silver/pit_stops\")\ndf_qualifying.write.format('delta').save(f\"/mnt/{storageAccountName}/silver/qualifying\")\ndf_races.write.format('delta').save(f\"/mnt/{storageAccountName}/silver/races\")\ndf_results.write.format('delta').save(f\"/mnt/{storageAccountName}/silver/results\")\ndf_seasons.write.format('delta').save(f\"/mnt/{storageAccountName}/silver/seasons\")\ndf_sprint_results.write.format('delta').save(f\"/mnt/{storageAccountName}/silver/sprint_results\")\ndf_status.write.format('delta').save(f\"/mnt/{storageAccountName}/silver/status\")\n</code></pre>"},{"location":"pipeline/silver/#verificando-os-dados-gravados-em-delta-na-camada-silver","title":"Verificando os dados gravados em delta na camada silver","text":"<pre><code>display(dbutils.fs.ls(f\"/mnt/{storageAccountName}/silver/\"))\n</code></pre>"}]}